<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning | Osama Sakhi</title>
    <link>/tags/machine-learning/</link>
      <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>machine learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Osama Sakhi Â© 2019</copyright><lastBuildDate>Mon, 30 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>machine learning</title>
      <link>/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>The REDUCE Project</title>
      <link>/project/reduce/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/reduce/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://socweb.cc.gatech.edu/projects/&#34;&gt;REDUCE&lt;/a&gt; project seeks to improve real-time estimation of suicide rates to better enable suicide prevention activities. We&#39;ve partnered with the CDC to use historical trends with social media data to forecast future trends week-to-week.&lt;/p&gt;
&lt;p&gt;One of the motivations for integrating social media data is the speed at which up-to-date social media can be obtained &amp;ndash; nearly instantly. In contrast, the CDC data we get will be a minimum of two years old because of all of the aggregation efforts needed by the CDC to publish this data.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;For my role in this project, I&#39;ve primarily worked on extracting meaninful signals from the social media data. Most of my time has been spent on the Twitter data we have, which comprises of over 9 million tweets focused around a selected set of keyword phrases related to depression, anxiety, and suicide.&lt;/p&gt;
&lt;p&gt;With the tweets, I have built various embeddings for each tweet. These embeddings are aggregated in various ways to produce a representation for a week&#39;s worth of tweets at a time, and that representation is then fed into various regressors. The best trained regressor will be used as one learner in our final ensemble approach, which will consist of learners for the other data sources we have, such as CDC suicide data, Google Health data, Reddit language model data, etc.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;This project is ongoing, so we don&#39;t have much to report just yet. What we can say, however, is that the models built from the tweets seems to be very capable in our initial results. For one of our models, we found that the representation of tweets at a weekly granularity produced a model that performed nearly as well as the models built with just historical data (which is limited and is only available every 2 years).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cross-Domain Context Prediction</title>
      <link>/project/crossdom/</link>
      <pubDate>Sat, 30 Nov 2019 12:00:00 +0000</pubDate>
      <guid>/project/crossdom/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Given a hand-drawn sketch, retrieve the image instance that this sketch was drawn for
&lt;img src=&#34;probstatement.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1505.05192&#34;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sketchy.eye.gatech.edu/paper.pdf&#34;&gt;The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231217314364#!&#34;&gt;Cross-modal Subspace Learning for fine-grained sketch-based image retrieval&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Aligned, paired images available. For this, we compute canny edges of the images in the PASCAL VOC dataset.&lt;/li&gt;
&lt;li&gt;Clustering image and sketch embeddings from a well-trained network will result in well-formed discrete clusters that are domain agnostic.&lt;/li&gt;
&lt;li&gt;The model that performs well on cross domain context prediction will perform well on the cross-domain image retrieval task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-text-task&#34;&gt;Pre-text Task&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We divide the image into 4 regions, with uneven spacing and jitter&lt;/li&gt;
&lt;li&gt;We then extract two patches, one from each domain, i.e. images from Pascal, and their Canny edges&lt;/li&gt;
&lt;li&gt;We finally compute the relative positioning of the patches using the context encoder&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;spatial.png&#34; alt=&#34;Spatial relation between patches&#34;&gt;
&lt;img src=&#34;classification.png&#34; alt=&#34;AlexNet-inspired classification architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;image-retrieval&#34;&gt;Image Retrieval&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We first compute embeddings for the query sketch using AlexNet trained on the pretext&lt;/li&gt;
&lt;li&gt;We then perform a nearest neighbour search on the embeddings from the dataset of images&lt;/li&gt;
&lt;li&gt;We retrieve the nearest 5 and 10 images for top-5 and top-10 similarity scores&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;neighbors.png&#34; alt=&#34;Computing embeddings and finding nearest neighbors&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;visual-results&#34;&gt;Visual Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;visual1.png&#34; alt=&#34;Good Results&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here is one of our &amp;ldquo;bad&amp;rdquo; results &amp;ndash; we can see that the correct instance image is present in the retrieved images&lt;/li&gt;
&lt;li&gt;We can also see that the other bird result also captures similar pose as the sketch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;visual2.png&#34; alt=&#34;Bad Results&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here is one of our &amp;ldquo;bad&amp;rdquo; results &amp;ndash; we see that the correct instance wasn&#39;t retrieved&lt;/li&gt;
&lt;li&gt;Additionally, the correct class wasn&#39;t retrieved either&lt;/li&gt;
&lt;li&gt;Note how despite the incorrect class/instance retrieval, we do see similarities in the pose and shape between the sketches and the retrieved images&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-to-baselines&#34;&gt;Comparison to Baselines&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;results_table.png&#34; alt=&#34;Results table. Our methodology beats out feature pyramids without any supervision.&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although our approach didn&#39;t beat out our main baseline, the Sketchy database approach, we were able to beat out the feature pyramid approach with no supervision&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Study the effects of further training on pretext task&lt;/li&gt;
&lt;li&gt;Use context-encoder as pretraining for supervised image retreival models&lt;/li&gt;
&lt;li&gt;Use more sophisticated feature extractors (like GoogLeNet or VGG) that more recent Sketch-Based Image Retrieval methods use&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SUMO Challenge</title>
      <link>/project/sumo/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/project/sumo/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In Fall 2018, I joined &lt;a href=&#34;http://cs.brown.edu/people/zr1/&#34;&gt;Zhile Ren&lt;/a&gt; and &lt;a href=&#34;(https://www.cc.gatech.edu/~dellaert/FrankDellaert/Frank_Dellaert/Frank_Dellaert.html)&#34;&gt;Frank Dellaert&lt;/a&gt; on the &lt;a href=&#34;https://sumochallenge.org&#34;&gt;SUMO Challenge&lt;/a&gt; by Facebook.&lt;/p&gt;
&lt;p&gt;The SUMO Challenge has several tracks for which we can compete for the best results, so our team decided to compete for the 3D Bounding Box track. That is, given 360 Degree RGB-Depth images, can we determine the 3D oriented bounding box for each of the items in the given scene?&lt;/p&gt;
&lt;p&gt;There are over 100 included object categories, with a pretty skewed distribution:
&lt;img src=&#34;/img/category_stats.png&#34; alt=&#34;Sumo Categories&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;approaches&#34;&gt;Approaches&lt;/h2&gt;
&lt;p&gt;At first, Zhile and I were determined to use direct 3D detection approaches like using &lt;a href=&#34;http://cs.brown.edu/people/zr1/publications/cvpr2016/cvpr2016.pdf&#34;&gt;Clouds of Oriented Gradients (CoG)&lt;/a&gt;. However, the enormous size of the dataset (1+ Terrabyte) and image sizes (&lt;code&gt;1024 x 6144 x 3 channels&lt;/code&gt;) immediately became problematic, both for training and inference.&lt;/p&gt;
&lt;p&gt;CoGs are great when the shape of the categories are easy to discern (i.e shapes that are not predominantly &amp;ldquo;box-like&amp;rdquo;), but in SUMO, we had many items that would have taken on very similar CoGs such as &lt;code&gt;single_bed&lt;/code&gt; versus &lt;code&gt;double_bed&lt;/code&gt;, &lt;code&gt;tv_stand&lt;/code&gt; versus &lt;code&gt;dresser&lt;/code&gt;, and so on.&lt;/p&gt;
&lt;p&gt;We decided a good starting point would be to determine object locations in the 2D space, and see where we could go from there. We intended to then train for CoGs, but we had limited time with the SUMO challenge deadline being mid-December, so we decided to see how well we could perform by using the following pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Project 3D bounding boxes into 2D space to generate a 2D dataset&lt;/li&gt;
&lt;li&gt;Train a Faster-R-CNN network to detect objects on the 2D dataset&lt;/li&gt;
&lt;li&gt;Project the 2D coordinates back into 3D, do some simple post-processing to prevent things like walls and floors from being in the bounding box&lt;/li&gt;
&lt;li&gt;Train Regressors to help correct the coordinates on a per-category basis&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;On December 18th, we found out that the SUMO challege deadline would be pushed back to January 14th. We continued work on step &lt;code&gt;(3)&lt;/code&gt; but didn&#39;t get very far in step &lt;code&gt;(4)&lt;/code&gt;, so our final pipeline consists of steps &lt;code&gt;(1)&lt;/code&gt; through &lt;code&gt;(3)&lt;/code&gt;. We made our submission on Sunday, January 13th.&lt;/p&gt;
&lt;p&gt;We found out that we were narrowly beat by the Princeton Vision Lab for 1st place, leaving us in 2nd place for our cateogry.&lt;/p&gt;
&lt;h2 id=&#34;visual-results-in-3d&#34;&gt;Visual Results in 3D&lt;/h2&gt;
&lt;p&gt;Below are our results in the 3D space. On the left, we have a red bounding box, which is our detection. On the right in green we have the ground truth bounding boxes for that same scene.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/sumo/sumo_tv.png&#34; alt=&#34;Detection of a television  with books in the tv stand&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;/img/sumo/sumo_shower.png&#34; alt=&#34;Detection of a shower&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;/img/sumo/sumo_door.png&#34; alt=&#34;Detection of a door&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;visual-results-in-2d&#34;&gt;Visual Results in 2D&lt;/h2&gt;
&lt;p&gt;Below are some visual results we&#39;ve gotten in the 2D space training the Faster-R-CNN network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/sumo/one.jpg&#34; alt=&#34;Scene 1&#34;&gt;
&lt;img src=&#34;/img/sumo/two.jpg&#34; alt=&#34;Scene 2&#34;&gt;
&lt;img src=&#34;/img/sumo/three.jpg&#34; alt=&#34;Scene 3&#34;&gt;
&lt;img src=&#34;/img/sumo/four.jpg&#34; alt=&#34;Scene 4&#34;&gt;
&lt;img src=&#34;/img/sumo/five.jpg&#34; alt=&#34;Scene 5&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the network performs fairly well. Mean AP was about &lt;code&gt;23%&lt;/code&gt; which is reasonable given the enormous dataset size and the skewed distribution. One aspect that particularly complicates this challenge are the walls and floors, which are object categories, but the coordinates are often outside of single-frame views, which is exactly what this network was trained to perform on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sumo</title>
      <link>/publication/sumo/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/publication/sumo/</guid>
      <description>&lt;h2 id=&#34;important&#34;&gt;IMPORTANT&lt;/h2&gt;
&lt;p&gt;This paper was being worked on until May 2019. At that time, the competition organizer, Facebook, faced a lawsuit for the creation of the dataset. To mitigate further risk of litigation, this paper was left unpublished and incomplete indefinitely.&lt;/p&gt;
&lt;p&gt;The abstract presented above is from our working draft of the paper.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
