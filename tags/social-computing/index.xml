<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>social computing | Osama Sakhi</title>
    <link>/tags/social-computing/</link>
      <atom:link href="/tags/social-computing/index.xml" rel="self" type="application/rss+xml" />
    <description>social computing</description>
<<<<<<< HEAD
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Osama Sakhi © 2019</copyright><lastBuildDate>Fri, 29 Nov 2019 00:00:00 +0000</lastBuildDate>
=======
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Osama Sakhi © 2019</copyright><lastBuildDate>Mon, 30 Dec 2019 00:00:00 +0000</lastBuildDate>
>>>>>>> Local
    <image>
      <url>/img/icon-192.png</url>
      <title>social computing</title>
      <link>/tags/social-computing/</link>
    </image>
    
    <item>
      <title>The REDUCE Project</title>
      <link>/project/reduce/</link>
<<<<<<< HEAD
      <pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/project/reduce/</guid>
      <description>

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&#34;approaches&#34;&gt;Approaches&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;
=======
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/reduce/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://socweb.cc.gatech.edu/projects/&#34;&gt;REDUCE&lt;/a&gt; project seeks to improve real-time estimation of suicide rates to better enable suicide prevention activities. We&#39;ve partnered with the CDC to use historical trends with social media data to forecast future trends week-to-week.&lt;/p&gt;
&lt;p&gt;One of the motivations for integrating social media data is the speed at which up-to-date social media can be obtained &amp;ndash; nearly instantly. In contrast, the CDC data we get will be a minimum of two years old because of all of the aggregation efforts needed by the CDC to publish this data.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;For my role in this project, I&#39;ve primarily worked on extracting meaninful signals from the social media data. Most of my time has been spent on the Twitter data we have, which comprises of over 9 million tweets focused around a selected set of keyword phrases related to depression, anxiety, and suicide.&lt;/p&gt;
&lt;p&gt;With the tweets, I have built various embeddings for each tweet. These embeddings are aggregated in various ways to produce a representation for a week&#39;s worth of tweets at a time, and that representation is then fed into various regressors. The best trained regressor will be used as one learner in our final ensemble approach, which will consist of learners for the other data sources we have, such as CDC suicide data, Google Health data, Reddit language model data, etc.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;This project is ongoing, so we don&#39;t have much to report just yet. What we can say, however, is that the models built from the tweets seems to be very capable in our initial results. For one of our models, we found that the representation of tweets at a weekly granularity produced a model that performed nearly as well as the models built with just historical data (which is limited and is only available every 2 years).&lt;/p&gt;
>>>>>>> Local
</description>
    </item>
    
  </channel>
</rss>
