<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Osama Sakhi</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Osama Sakhi</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Osama Sakhi © 2019</copyright><lastBuildDate>Sat, 30 Nov 2019 12:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Osama Sakhi</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Pathfinding</title>
      <link>/teaching/tip/pathfinding/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      <guid>/teaching/tip/pathfinding/</guid>
      <description>

&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;

&lt;p&gt;Here are links to the projects I used for my students during the Pathfinding portion of the course:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ai.berkeley.edu/search.html&#34; target=&#34;_blank&#34;&gt;Pacman Search Project&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>/teaching/tip/ml/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      <guid>/teaching/tip/ml/</guid>
      <description>

&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;

&lt;p&gt;Here are links to the repos/projects I created or used for my students during the Machine Learning portion of the course:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml&#34; target=&#34;_blank&#34;&gt;Repository for AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml/blob/master/machine_learning_walkthrough.ipynb&#34; target=&#34;_blank&#34;&gt;Machine Learning Experiment Walkthrough&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml/blob/master/cross_validation_tutorial.ipynb&#34; target=&#34;_blank&#34;&gt;Cross-Validation Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml/blob/master/naive_bayes.ipynb&#34; target=&#34;_blank&#34;&gt;Naive Bayes by Hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml/blob/master/neural_networks_by_hand.ipynb&#34; target=&#34;_blank&#34;&gt;Neural Networks by Hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml/blob/master/regression_tutorial.ipynb&#34; target=&#34;_blank&#34;&gt;Regression Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>/teaching/tip/nlp/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      <guid>/teaching/tip/nlp/</guid>
      <description>

&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;

&lt;p&gt;Here are links to the projects I created or used for my students during the Natural Language Processing portion of the course:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mosdragon/teach_ml/blob/master/amazon_reviews_classification.ipynb&#34; target=&#34;_blank&#34;&gt;Sentiment Analysis of Amazon Reviews&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cross-Domain Context Prediction</title>
      <link>/project/crossdom/</link>
      <pubDate>Sat, 30 Nov 2019 12:00:00 +0000</pubDate>
      <guid>/project/crossdom/</guid>
      <description>

&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Given a hand-drawn sketch, retrieve the image instance that this sketch was drawn for
&lt;img src=&#34;probstatement.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1505.05192&#34; target=&#34;_blank&#34;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sketchy.eye.gatech.edu/paper.pdf&#34; target=&#34;_blank&#34;&gt;The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231217314364#!&#34; target=&#34;_blank&#34;&gt;Cross-modal Subspace Learning for fine-grained sketch-based image retrieval&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;

&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Aligned, paired images available. For this, we compute canny edges of the images in the PASCAL VOC dataset.&lt;/li&gt;
&lt;li&gt;Clustering image and sketch embeddings from a well-trained network will result in well-formed discrete clusters that are domain agnostic.&lt;/li&gt;
&lt;li&gt;The model that performs well on cross domain context prediction will perform well on the cross-domain image retrieval task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;pre-text-task&#34;&gt;Pre-text Task&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We divide the image into 4 regions, with uneven spacing and jitter&lt;/li&gt;
&lt;li&gt;We then extract two patches, one from each domain, i.e. images from Pascal, and their Canny edges&lt;/li&gt;
&lt;li&gt;We finally compute the relative positioning of the patches using the context encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;spatial.png&#34; alt=&#34;Spatial relation between patches&#34; /&gt;
&lt;img src=&#34;classification.png&#34; alt=&#34;AlexNet-inspired classification architecture&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;image-retrieval&#34;&gt;Image Retrieval&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We first compute embeddings for the query sketch using AlexNet trained on the pretext&lt;/li&gt;
&lt;li&gt;We then perform a nearest neighbour search on the embeddings from the dataset of images&lt;/li&gt;
&lt;li&gt;We retrieve the nearest 5 and 10 images for top-5 and top-10 similarity scores&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;neighbors.png&#34; alt=&#34;Computing embeddings and finding nearest neighbors&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h3 id=&#34;visual-results&#34;&gt;Visual Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;visual1.png&#34; alt=&#34;Good Results&#34; /&gt;
- &lt;strong&gt;Across all classes&lt;/strong&gt; → Top two images retrieved correctly fetch birds with the correct pose&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;visual2.png&#34; alt=&#34;Bad Results&#34; /&gt;
- &lt;strong&gt;Across all classes&lt;/strong&gt; → Even in incorrect retrievals across classes, the pose and shape seem to match the intended object&lt;/p&gt;

&lt;h3 id=&#34;comparison-to-baselines&#34;&gt;Comparison to Baselines&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;results_table.png&#34; alt=&#34;Results table. Our methodology beats out feature pyramids without any supervision.&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The REDUCE Project</title>
      <link>/project/reduce/</link>
      <pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/project/reduce/</guid>
      <description>

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&#34;approaches&#34;&gt;Approaches&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;TODO&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generative Adversarial Networks Discussion</title>
      <link>/talk/gans/</link>
      <pubDate>Thu, 28 Nov 2019 13:00:00 +0000</pubDate>
      <guid>/talk/gans/</guid>
      <description>

&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;

&lt;h1 id=&#34;slides&#34;&gt;Slides&lt;/h1&gt;

&lt;p&gt;TODO
&lt;!-- TODO --&gt;&lt;/p&gt;

&lt;!-- Slides can be added in a few ways: --&gt;

&lt;!-- - **Create** slides using Academic&#39;s [*Slides*](https://sourcethemes.com/academic/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file
- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>SUMO Challenge</title>
      <link>/project/sumo/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/project/sumo/</guid>
      <description>

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;In Fall 2018, I joined &lt;a href=&#34;http://cs.brown.edu/people/zr1/&#34; target=&#34;_blank&#34;&gt;Zhile Ren&lt;/a&gt; and &lt;a href=&#34;(https://www.cc.gatech.edu/~dellaert/FrankDellaert/Frank_Dellaert/Frank_Dellaert.html)&#34; target=&#34;_blank&#34;&gt;Frank Dellaert&lt;/a&gt; on the &lt;a href=&#34;https://sumochallenge.org&#34; target=&#34;_blank&#34;&gt;SUMO Challenge&lt;/a&gt; by Facebook.&lt;/p&gt;

&lt;p&gt;The SUMO Challenge has several tracks for which we can compete for the best results, so our team decided to compete for the 3D Bounding Box track. That is, given 360 Degree RGB-Depth images, can we determine the 3D oriented bounding box for each of the items in the given scene?&lt;/p&gt;

&lt;p&gt;There are over 100 included object categories, with a pretty skewed distribution:
&lt;img src=&#34;/img/category_stats.png&#34; alt=&#34;Sumo Categories&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;approaches&#34;&gt;Approaches&lt;/h2&gt;

&lt;p&gt;At first, Zhile and I were determined to use direct 3D detection approaches like using &lt;a href=&#34;http://cs.brown.edu/people/zr1/publications/cvpr2016/cvpr2016.pdf&#34; target=&#34;_blank&#34;&gt;Clouds of Oriented Gradients (CoG)&lt;/a&gt;. However, the enormous size of the dataset (1+ Terrabyte) and image sizes (&lt;code&gt;1024 x 6144 x 3 channels&lt;/code&gt;) immediately became problematic, both for training and inference.&lt;/p&gt;

&lt;p&gt;CoGs are great when the shape of the categories are easy to discern (i.e shapes that are not predominantly &amp;ldquo;box-like&amp;rdquo;), but in SUMO, we had many items that would have taken on very similar CoGs such as &lt;code&gt;single_bed&lt;/code&gt; versus &lt;code&gt;double_bed&lt;/code&gt;, &lt;code&gt;tv_stand&lt;/code&gt; versus &lt;code&gt;dresser&lt;/code&gt;, and so on.&lt;/p&gt;

&lt;p&gt;We decided a good starting point would be to determine object locations in the 2D space, and see where we could go from there. We intended to then train for CoGs, but we had limited time with the SUMO challenge deadline being mid-December, so we decided to see how well we could perform by using the following pipeline:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Project 3D bounding boxes into 2D space to generate a 2D dataset&lt;/li&gt;
&lt;li&gt;Train a Faster-R-CNN network to detect objects on the 2D dataset&lt;/li&gt;
&lt;li&gt;Project the 2D coordinates back into 3D, do some simple post-processing to prevent things like walls and floors from being in the bounding box&lt;/li&gt;
&lt;li&gt;Train Regressors to help correct the coordinates on a per-category basis&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;On December 18th, we found out that the SUMO challege deadline would be pushed back to January 14th. We continued work on step &lt;code&gt;(3)&lt;/code&gt; but didn&amp;rsquo;t get very far in step &lt;code&gt;(4)&lt;/code&gt;, so our final pipeline consists of steps &lt;code&gt;(1)&lt;/code&gt; through &lt;code&gt;(3)&lt;/code&gt;. We made our submission on Sunday, January 13th.&lt;/p&gt;

&lt;p&gt;We found out that we were narrowly beat by the Princeton Vision Lab for 1st place, leaving us in 2nd place for our cateogry.&lt;/p&gt;

&lt;h2 id=&#34;visual-results-in-3d&#34;&gt;Visual Results in 3D&lt;/h2&gt;

&lt;p&gt;Below are our results in the 3D space. On the left, we have a red bounding box, which is our detection. On the right in green we have the ground truth bounding boxes for that same scene.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/sumo/sumo_tv.png&#34; alt=&#34;Detection of a television  with books in the tv stand&#34; /&gt;
&lt;center&gt; &lt;strong&gt;Figure 1:&lt;/strong&gt; Detection of a &lt;code&gt;television&lt;/code&gt;  with &lt;code&gt;books&lt;/code&gt; in the tv stand. &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/sumo/sumo_shower.png&#34; alt=&#34;Detection of a shower&#34; /&gt;
&lt;center&gt; &lt;strong&gt;Figure 2:&lt;/strong&gt; Detection of a &lt;code&gt;shower&lt;/code&gt;. &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/sumo/sumo_door.png&#34; alt=&#34;Detection of a door&#34; /&gt;
&lt;center&gt; &lt;strong&gt;Figure 3:&lt;/strong&gt; Detection of a &lt;code&gt;door&lt;/code&gt;. &lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;visual-results-in-2d&#34;&gt;Visual Results in 2D&lt;/h2&gt;

&lt;p&gt;Below are some visual results we&amp;rsquo;ve gotten in the 2D space training the Faster-R-CNN network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/sumo/one.jpg&#34; alt=&#34;Scene 1&#34; /&gt;
&lt;img src=&#34;/img/sumo/two.jpg&#34; alt=&#34;Scene 2&#34; /&gt;
&lt;img src=&#34;/img/sumo/three.jpg&#34; alt=&#34;Scene 3&#34; /&gt;
&lt;img src=&#34;/img/sumo/four.jpg&#34; alt=&#34;Scene 4&#34; /&gt;
&lt;img src=&#34;/img/sumo/five.jpg&#34; alt=&#34;Scene 5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the network performs fairly well. Mean AP was about &lt;code&gt;23%&lt;/code&gt; which is reasonable given the enormous dataset size and the skewed distribution. One aspect that particularly complicates this challenge are the walls and floors, which are object categories, but the coordinates are often outside of single-frame views, which is exactly what this network was trained to perform on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Betweenness Centrality for Streaming Graphs</title>
      <link>/project/hpc/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/project/hpc/</guid>
      <description>

&lt;h2 id=&#34;background&#34;&gt;Background:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_theory&#34; target=&#34;_blank&#34;&gt;Graph Theory&lt;/a&gt; was among my favorite topics during my undergraduate studies, and in Fall 2016, I managed to find research that would build on my interests in that area as well as leverage the Systems background I had (from one of concentration areas: &lt;em&gt;Devices&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;I joined &lt;a href=&#34;https://www.cc.gatech.edu/~ogreen/&#34; target=&#34;_blank&#34;&gt;Dr. Oded Green&lt;/a&gt; that term, and we began work immediately on &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_connectivity&#34; target=&#34;_blank&#34;&gt;Streaming Graphs&lt;/a&gt; (also known as Dynamic Graphs). These are graphs where the Nodes $V$ and Edges $E$ change over time.&lt;/p&gt;

&lt;h2 id=&#34;problem-betweenness-centrality-on-streaming-graphs&#34;&gt;Problem: Betweenness Centrality on Streaming Graphs&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Betweenness_centrality&#34; target=&#34;_blank&#34;&gt;Betweenness Centrality&lt;/a&gt;(BC) is a measure of a node&amp;rsquo;s centrality in a graph based on it&amp;rsquo;s presence in shortest paths.&lt;/p&gt;

&lt;p&gt;Think of it as the &lt;em&gt;hub&lt;/em&gt; that connects many different transportation lines. The more dependent other transportation lines are on a given &lt;em&gt;hub&lt;/em&gt;, the more central that &lt;em&gt;hub&lt;/em&gt; is to the network.&lt;/p&gt;

&lt;p&gt;On a static graph, we can compute the BC values for each node pretty easily (in terms of time complexity). However, real-world graphs like Facebook&amp;rsquo;s Social Network Graph contain hundreds of millions of nodes and billions of edges. If we wanted up-to-date BC values for graphs of this scale, re-computing BC each time a node/edge is &lt;em&gt;inserted&lt;/em&gt; or &lt;em&gt;delete&lt;/em&gt; becomes infeasible.&lt;/p&gt;

&lt;p&gt;So that&amp;rsquo;s our task: Given a graph and then a sequence of insertions/deletions, can we accurately compute the BC values without recomputing on the entire graph?&lt;/p&gt;

&lt;h2 id=&#34;approach&#34;&gt;Approach:&lt;/h2&gt;

&lt;p&gt;We created a framework called &lt;a href=&#34;https://github.com/cuStinger/cuStinger&#34; target=&#34;_blank&#34;&gt;cuStinger&lt;/a&gt; (a &lt;em&gt;portmanteau&lt;/em&gt; of &lt;em&gt;Cuda&lt;/em&gt; and &lt;em&gt;Stinger&lt;/em&gt;), which served as the backbone of our adventure in getting Streaming BC going.&lt;/p&gt;

&lt;p&gt;Using this framework (which saved us largely from writing repetitive Cuda code), we implemented the algorithm as described &lt;a href=&#34;https://scholar.google.com/citations?user=C_7l2roAAAAJ&amp;amp;hl=en#d=gs_md_cita-d&amp;amp;p=&amp;amp;u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DC_7l2roAAAAJ%26citation_for_view%3DC_7l2roAAAAJ%3A9yKSN-GCB0IC%26tzom%3D300&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;challenges&#34;&gt;Challenges:&lt;/h2&gt;

&lt;p&gt;Since we were using Nvidia GPUs and Cuda under the hood, we had various challenges with race conditions, mutex locks, and validating results.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results:&lt;/h2&gt;

&lt;p&gt;By the end of that year, we managed to have a running version that handled &lt;em&gt;most&lt;/em&gt; edge cases of streaming graphs. There were some we hadn&amp;rsquo;t implemented yet when I left the lab.&lt;/p&gt;

&lt;p&gt;A few weeks later, the lab moved onto a new framework (developed in-house with Oded and his lab), &lt;a href=&#34;https://github.com/hornet-gt/hornet&#34; target=&#34;_blank&#34;&gt;Hornet&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Parallel Computation comes with some challenges, be ready&lt;/li&gt;
&lt;li&gt;Advances in GPU programming are opening doors to many new areas ripe for exploration&lt;/li&gt;
&lt;li&gt;ALWAYS write tests to validate results (we compared our Streaming BC results to our Static BC at each insertion/deletion)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tools-used&#34;&gt;Tools Used&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;C/C++&lt;/li&gt;
&lt;li&gt;Cuda&lt;/li&gt;
&lt;li&gt;cuStinger framework&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Speaking at TEDxGeorgiaTech&#39;s Fall 2018 Speaker Salon</title>
      <link>/post/ted/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/ted/</guid>
      <description>&lt;p&gt;In Fall 2018, I decided to apply to be a speaker for &lt;a href=&#34;https://www.tedxgeorgiatech.com&#34;&gt;TEDxGeorgiaTech&lt;/a&gt;&#39;s Fall speaker salon. I was chosen as one of the 6 speakers among 70 applicants.&lt;/p&gt;

&lt;p&gt;For the 6 weeks that followed, I rewrote my speech several times over. I changed it drastically from what it started off as, added entirely new ideas, and overall realized that the focus of the talk had to be not my experience, but rather what others should take from it.&lt;/p&gt;

&lt;p&gt;Finally, on November 4th, 2018, my hard work paid off. I was the final speaker for the night, so I ended the show. My talk, &lt;em&gt;Compounding Interest: How Instilling Drive Goes a Long Way&lt;/em&gt; tried to strike a delicate balance between being informative and being relatable. My focus was on the importance of instilling drive and the cascading effect that it had on educating society.&lt;/p&gt;

&lt;p&gt;You can find my full talk &lt;a href=&#34;https://www.youtube.com/watch?v=0FWfsGd0GWc&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Additionally, I found out in February, 2019 that I was chosen among the 6 speakers from last Fall to speak again at the TedXGeorgiaTech Conference held on April 13th, 2019. I&#39;m was ecstatic about the chance to revise my talk and give it one more time in front of an even larger crowd!&lt;/p&gt;

&lt;p&gt;The revised talk was posted &lt;a href=&#34;https://youtu.be/pH9I2ZnH08Y&#34;&gt;here&lt;/a&gt; in late July.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sumo</title>
      <link>/publication/sumo/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/publication/sumo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
